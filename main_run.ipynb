{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from trainer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the config files\n",
    "In this example, we will apply a Vanilla Neural Network to a setting of one store under a lost demand assumption.\n",
    "Go to the respective config files to change the hyperparameters of the neural network or the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_setting_file = 'config_files/settings/one_store_real_data_lost_demand.yml'\n",
    "config_hyperparams_file = 'config_files/policies_and_hyperparams/vanilla_one_store.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adm/dev/hpml-neural-inventory-control/data_handling.py:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  demand = torch.load(demand_params['file_location'])[: self.num_samples]\n",
      "/Users/adm/dev/hpml-neural-inventory-control/data_handling.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(demand)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch.compile in default mode\n"
     ]
    }
   ],
   "source": [
    "from wandb_trainer import WandbTrainer\n",
    "\n",
    "with open(config_setting_file, 'r') as file:\n",
    "    config_setting = yaml.safe_load(file)\n",
    "\n",
    "with open(config_hyperparams_file, 'r') as file:\n",
    "    config_hyperparams = yaml.safe_load(file)\n",
    "\n",
    "setting_keys = 'seeds', 'test_seeds', 'problem_params', 'params_by_dataset', 'observation_params', 'store_params', 'warehouse_params', 'echelon_params', 'sample_data_params'\n",
    "hyperparams_keys = 'trainer_params', 'optimizer_params', 'nn_params'\n",
    "seeds, test_seeds, problem_params, params_by_dataset, observation_params, store_params, warehouse_params, echelon_params, sample_data_params = [\n",
    "    config_setting[key] for key in setting_keys\n",
    "    ]\n",
    "\n",
    "trainer_params, optimizer_params, nn_params = [config_hyperparams[key] for key in hyperparams_keys]\n",
    "observation_params = DefaultDict(lambda: None, observation_params)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset_creator = DatasetCreator()\n",
    "\n",
    "# For realistic data, train, dev and test sets correspond to the same products, but over disjoint periods.\n",
    "# We will therefore create one scenario, and then split the data into train, dev and test sets by \n",
    "# \"copying\" all non-period related information, and then splitting the period related information\n",
    "if sample_data_params['split_by_period']:\n",
    "    \n",
    "    scenario = Scenario(\n",
    "        periods=None,  # period info for each dataset is given in sample_data_params\n",
    "        problem_params=problem_params, \n",
    "        store_params=store_params, \n",
    "        warehouse_params=warehouse_params, \n",
    "        echelon_params=echelon_params, \n",
    "        num_samples=params_by_dataset['train']['n_samples'],  # in this case, num_samples=number of products, which has to be the same across all datasets\n",
    "        observation_params=observation_params, \n",
    "        seeds=seeds\n",
    "        )\n",
    "    \n",
    "    train_dataset, dev_dataset, test_dataset = dataset_creator.create_datasets(\n",
    "        scenario, \n",
    "        split=True, \n",
    "        by_period=True, \n",
    "        periods_for_split=[sample_data_params[k] for  k in ['train_periods', 'dev_periods', 'test_periods']],)\n",
    "\n",
    "# For synthetic data, we will first create a scenario that we will divide into train and dev sets by sample index.\n",
    "# Then, we will create a separate scenario for the test set, which will be exaclty the same as the previous scenario, \n",
    "# but with different seeds to generate demand traces, and with a longer time horizon.\n",
    "# One can use this method of generating scenarios to train a model using some specific problem primitives, \n",
    "# and then test it on a different set of problem primitives, by simply creating a new scenario with the desired primitives.\n",
    "else:\n",
    "    scenario = Scenario(\n",
    "        periods=params_by_dataset['train']['periods'], \n",
    "        problem_params=problem_params, \n",
    "        store_params=store_params, \n",
    "        warehouse_params=warehouse_params, \n",
    "        echelon_params=echelon_params, \n",
    "        num_samples=params_by_dataset['train']['n_samples'] + params_by_dataset['dev']['n_samples'], \n",
    "        observation_params=observation_params, \n",
    "        seeds=seeds\n",
    "        )\n",
    "\n",
    "    train_dataset, dev_dataset = dataset_creator.create_datasets(scenario, split=True, by_sample_indexes=True, sample_index_for_split=params_by_dataset['dev']['n_samples'])\n",
    "\n",
    "    scenario = Scenario(\n",
    "        params_by_dataset['test']['periods'], \n",
    "        problem_params, \n",
    "        store_params, \n",
    "        warehouse_params, \n",
    "        echelon_params, \n",
    "        params_by_dataset['test']['n_samples'], \n",
    "        observation_params, \n",
    "        test_seeds\n",
    "        )\n",
    "\n",
    "    test_dataset = dataset_creator.create_datasets(scenario, split=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=params_by_dataset['train']['batch_size'], shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=params_by_dataset['dev']['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params_by_dataset['test']['batch_size'], shuffle=False)\n",
    "data_loaders = {'train': train_loader, 'dev': dev_loader, 'test': test_loader}\n",
    "\n",
    "neural_net_creator = NeuralNetworkCreator\n",
    "model = neural_net_creator().create_neural_network(scenario, nn_params, device=device)\n",
    "\n",
    "loss_function = PolicyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_params['learning_rate'])\n",
    "\n",
    "simulator = Simulator(device=device)\n",
    "# trainer = Trainer(device=device)\n",
    "trainer = WandbTrainer(device=device)\n",
    "\n",
    "# We will create a folder for each day of the year, and a subfolder for each model\n",
    "# When executing with different problem primitives (i.e. instance), it might be useful to create an additional subfolder for each instance\n",
    "trainer_params['base_dir'] = 'saved_models'\n",
    "trainer_params['save_model_folders'] = [trainer.get_year_month_day(), nn_params['name']]\n",
    "\n",
    "# We will simply name the model with the current time stamp\n",
    "trainer_params['save_model_filename'] = trainer.get_time_stamp()\n",
    "\n",
    "# Load previous model if load_model is set to True in the config file\n",
    "if trainer_params['load_previous_model']:\n",
    "    print(f'Loading model from {trainer_params[\"load_model_path\"]}')\n",
    "    model, optimizer = trainer.load_model(model, optimizer, trainer_params['load_model_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/adm/dev/hpml-neural-inventory-control/wandb/run-20241214_164625-eshjtp9t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adm/neural-inventory-control/runs/eshjtp9t' target=\"_blank\">vanilla_one_store_vanilla_one_store</a></strong> to <a href='https://wandb.ai/adm/neural-inventory-control' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adm/neural-inventory-control' target=\"_blank\">https://wandb.ai/adm/neural-inventory-control</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adm/neural-inventory-control/runs/eshjtp9t' target=\"_blank\">https://wandb.ai/adm/neural-inventory-control/runs/eshjtp9t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "time (s): 22.734985828399658\n",
      "epoch: 1\n",
      "Average per-period train loss: -13.544366663152521\n",
      "Average per-period dev loss: -19.376280391917508\n",
      "\n",
      "Epoch 1\n",
      "time (s): 9.046860933303833\n",
      "epoch: 2\n",
      "Average per-period train loss: -24.14897884022106\n",
      "Average per-period dev loss: -38.821370741900274\n",
      "\n",
      "Epoch 2\n",
      "time (s): 6.2310662269592285\n",
      "epoch: 3\n",
      "Average per-period train loss: -37.75007282603871\n",
      "Average per-period dev loss: -49.65972002814798\n",
      "\n",
      "Epoch 3\n",
      "time (s): 8.250066995620728\n",
      "epoch: 4\n",
      "Average per-period train loss: -59.2833779074929\n",
      "Average per-period dev loss: -84.41437306123622\n",
      "\n",
      "Epoch 4\n",
      "time (s): 5.7465410232543945\n",
      "epoch: 5\n",
      "Average per-period train loss: -112.3592792857777\n",
      "Average per-period dev loss: -128.7024554084329\n",
      "\n",
      "Epoch 5\n",
      "time (s): 4.569264888763428\n",
      "epoch: 6\n",
      "Average per-period train loss: -174.1952098499645\n",
      "Average per-period dev loss: -167.82614315257354\n",
      "\n",
      "Epoch 6\n",
      "time (s): 6.0567991733551025\n",
      "epoch: 7\n",
      "Average per-period train loss: -195.23017467151988\n",
      "Average per-period dev loss: -176.8274320714614\n",
      "\n",
      "Epoch 7\n",
      "time (s): 3.9024410247802734\n",
      "epoch: 8\n",
      "Average per-period train loss: -202.46265203302556\n",
      "Average per-period dev loss: -190.4802748736213\n",
      "\n",
      "Epoch 8\n",
      "time (s): 3.8176510334014893\n",
      "epoch: 9\n",
      "Average per-period train loss: -210.12612637606534\n",
      "Average per-period dev loss: -197.5704848345588\n",
      "\n",
      "Epoch 9\n",
      "time (s): 4.062884092330933\n",
      "epoch: 10\n",
      "Average per-period train loss: -218.62355180220172\n",
      "Average per-period dev loss: -201.74423397288604\n",
      "\n",
      "Epoch 10\n",
      "time (s): 4.158420085906982\n",
      "epoch: 11\n",
      "Average per-period train loss: -224.62148215553978\n",
      "Average per-period dev loss: -201.35423009535845\n",
      "\n",
      "Epoch 11\n",
      "time (s): 4.563846826553345\n",
      "epoch: 12\n",
      "Average per-period train loss: -226.33021129261363\n",
      "Average per-period dev loss: -203.1891264073989\n",
      "\n",
      "Epoch 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hpml-neural-inventory-control/wandb_trainer.py:104\u001b[0m, in \u001b[0;36mWandbTrainer.train\u001b[0;34m(self, epochs, loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, trainer_params, use_wandb)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m trainer_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_dev_every_n_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_wandb):\n\u001b[0;32m--> 104\u001b[0m         average_dev_loss, average_dev_loss_to_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m            \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperiods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_periods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore_periods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     average_dev_loss, average_dev_loss_to_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/dev/hpml-neural-inventory-control/wandb_trainer.py:144\u001b[0m, in \u001b[0;36mWandbTrainer.do_one_epoch\u001b[0;34m(self, optimizer, data_loader, loss_function, simulator, model, periods, problem_params, observation_params, train, ignore_periods, discrete_allocation)\u001b[0m\n\u001b[1;32m    141\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    142\u001b[0m periods_tracking_loss \u001b[38;5;241m=\u001b[39m periods \u001b[38;5;241m-\u001b[39m ignore_periods\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, wandb\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    146\u001b[0m         data_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmove_batch_to_device(data_batch)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neural_inventory/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neural_inventory/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    trainer_params['epochs'], \n",
    "    loss_function, simulator, \n",
    "    model, \n",
    "    data_loaders, \n",
    "    optimizer, \n",
    "    problem_params, \n",
    "    observation_params, \n",
    "    params_by_dataset, \n",
    "    trainer_params,\n",
    "    use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "ymin, ymax = 0, 20  # set the y-axis limit for the plot\n",
    "# trainer.plot_losses(ymin=ymin, ymax=ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_test_loss, average_test_loss_to_report = trainer.test(\n",
    "    loss_function, \n",
    "    simulator, \n",
    "    model, \n",
    "    data_loaders, \n",
    "    optimizer, \n",
    "    problem_params, \n",
    "    observation_params, \n",
    "    params_by_dataset, \n",
    "    discrete_allocation=store_params['demand']['distribution'] == 'poisson',\n",
    "    use_wandb=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average per-period test loss: {average_test_loss_to_report}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_inventory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
